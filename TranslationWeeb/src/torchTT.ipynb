{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"downloaded = False","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install fugashi[unidic-lite]\n!pip install fugashi[unidic]\n!pip install spacy\nif not downloaded:\n    !python -m unidic download\n    !python -m spacy download ja_core_news_sm\n    !python -m spacy download en_core_web_sm\n    downloaded = True\nimport xml.etree.ElementTree as ET\nimport os\nimport time\nimport fugashi\nimport nltk\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchtext.data import Field, BucketIterator, TabularDataset\nimport spacy\nimport random\nimport math\nimport gensim\nfrom torchtext.data.metrics import bleu_score\nfrom gensim.models import Word2Vec","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting fugashi[unidic]\n  Downloading fugashi-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (486 kB)\n\u001b[K     |████████████████████████████████| 486 kB 2.9 MB/s eta 0:00:01\n\u001b[?25hCollecting unidic\n  Downloading unidic-1.0.3.tar.gz (5.1 kB)\nRequirement already satisfied: requests<3.0.0,>=2.22.0 in /opt/conda/lib/python3.7/site-packages (from unidic->fugashi[unidic]) (2.25.1)\nRequirement already satisfied: tqdm<5.0.0,>=4.41.1 in /opt/conda/lib/python3.7/site-packages (from unidic->fugashi[unidic]) (4.55.1)\nRequirement already satisfied: wasabi<1.0.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from unidic->fugashi[unidic]) (0.8.2)\nRequirement already satisfied: plac<2.0.0,>=1.1.3 in /opt/conda/lib/python3.7/site-packages (from unidic->fugashi[unidic]) (1.1.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.22.0->unidic->fugashi[unidic]) (1.26.2)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.22.0->unidic->fugashi[unidic]) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.22.0->unidic->fugashi[unidic]) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.22.0->unidic->fugashi[unidic]) (2.10)\nBuilding wheels for collected packages: unidic\n  Building wheel for unidic (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for unidic: filename=unidic-1.0.3-py3-none-any.whl size=5498 sha256=0855fc73603b1378f327e6533b76e570c6830fe3f2277f18e50b09d870898a4c\n  Stored in directory: /root/.cache/pip/wheels/23/30/0b/128289fb595ef4117d2976ffdbef5069ef83be813e88caa0a6\nSuccessfully built unidic\nInstalling collected packages: unidic, fugashi\nSuccessfully installed fugashi-1.1.0 unidic-1.0.3\nRequirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (2.3.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy) (49.6.0.post20201009)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.25.1)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.8.2)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.7.4)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.0.5)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.19.5)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.1.3)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.0.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (4.55.1)\nRequirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy) (7.4.5)\nRequirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\ndownload url: https://cotonoha-dic.s3-ap-northeast-1.amazonaws.com/unidic.zip\nDictionary version: 2.3.0+2020-10-08\nDownloading UniDic v2.3.0+2020-10-08...\nunidic.zip: 100%|████████████████████████████| 608M/608M [00:42<00:00, 14.3MB/s]\nFinished download.\nDownloaded UniDic v2.3.0+2020-10-08 to /opt/conda/lib/python3.7/site-packages/unidic/dicdir\nCollecting ja_core_news_sm==2.3.2\n  Downloading https://github.com/explosion/spacy-models/releases/download/ja_core_news_sm-2.3.2/ja_core_news_sm-2.3.2.tar.gz (7.6 MB)\n\u001b[K     |████████████████████████████████| 7.6 MB 5.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from ja_core_news_sm==2.3.2) (2.3.5)\nCollecting sudachipy>=0.4.5\n  Downloading SudachiPy-0.5.1.tar.gz (69 kB)\n\u001b[K     |████████████████████████████████| 69 kB 2.6 MB/s eta 0:00:011\n\u001b[?25hCollecting sudachidict_core>=20200330\n  Downloading SudachiDict-core-20201223.post1.tar.gz (8.8 kB)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (1.0.5)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (1.1.3)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (2.25.1)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (3.0.5)\nRequirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (7.4.5)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (1.19.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (49.6.0.post20201009)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (0.8.2)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (2.0.5)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (0.7.4)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (1.0.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (1.0.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (4.55.1)\nRequirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (3.3.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (3.4.0)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (3.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (1.26.2)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->ja_core_news_sm==2.3.2) (2020.12.5)\n","name":"stdout"},{"output_type":"stream","text":"Collecting sortedcontainers~=2.1.0\n  Downloading sortedcontainers-2.1.0-py2.py3-none-any.whl (28 kB)\nCollecting dartsclone~=0.9.0\n  Downloading dartsclone-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (473 kB)\n\u001b[K     |████████████████████████████████| 473 kB 9.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: Cython in /opt/conda/lib/python3.7/site-packages (from dartsclone~=0.9.0->sudachipy>=0.4.5->ja_core_news_sm==2.3.2) (0.29.21)\nBuilding wheels for collected packages: ja-core-news-sm, sudachidict-core, sudachipy\n  Building wheel for ja-core-news-sm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ja-core-news-sm: filename=ja_core_news_sm-2.3.2-py3-none-any.whl size=7572681 sha256=5fe295e0d1b5e39e343eb79057ae2bb78c201014d5fd2c88f8522c6a707a9cfe\n  Stored in directory: /tmp/pip-ephem-wheel-cache-wo5box7j/wheels/eb/24/fa/ca64ace11b329d1158b5a5300ff5f3a27ceebdb09852194bac\n  Building wheel for sudachidict-core (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sudachidict-core: filename=SudachiDict_core-20201223.post1-py3-none-any.whl size=71405681 sha256=65cb1ee647bdaa94daad5319b571e4182772e779f402fe73e2ca1072282255b4\n  Stored in directory: /tmp/pip-ephem-wheel-cache-wo5box7j/wheels/06/79/40/f404626b8327cceb1c1756e6b79b0dd9c3ad79c9d13aed7644\n  Building wheel for sudachipy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sudachipy: filename=SudachiPy-0.5.1-cp37-cp37m-linux_x86_64.whl size=911872 sha256=906fbdd7e48634da0862082c445011694aa73b3d8077b302fb9d9cb261ef62a1\n  Stored in directory: /tmp/pip-ephem-wheel-cache-wo5box7j/wheels/6d/00/62/d1337cfacee73930ca5a3162508f03df43ba2110effa673f44\nSuccessfully built ja-core-news-sm sudachidict-core sudachipy\nInstalling collected packages: sortedcontainers, dartsclone, sudachipy, sudachidict-core, ja-core-news-sm\n  Attempting uninstall: sortedcontainers\n    Found existing installation: sortedcontainers 2.3.0\n    Uninstalling sortedcontainers-2.3.0:\n      Successfully uninstalled sortedcontainers-2.3.0\nSuccessfully installed dartsclone-0.9.0 ja-core-news-sm-2.3.2 sortedcontainers-2.1.0 sudachidict-core-20201223.post1 sudachipy-0.5.1\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('ja_core_news_sm')\nCollecting en_core_web_sm==2.3.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n\u001b[K     |████████████████████████████████| 12.0 MB 5.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.2)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\nRequirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.55.1)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.6.0.post20201009)\nRequirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('en_core_web_sm')\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Wikipedia Corpus TSV Generation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/1912434/how-do-i-parse-xml-in-python\n# https://docs.python.org/3/library/xml.etree.elementtree.html\n# corpus = xml.etree.ElementTree.fromstring(all_of_it, parser=xml.etree.ElementTree.XMLParser(encoding='utf-8')).text\ndef createEnglishJapanesePairList(filepath, output) -> list:\n    print(f'creating english japanese tsv (filepath:{filepath})')\n\n    with open(output, mode=\"w\", encoding=\"utf-8\") as file_in:\n        for path, subdirs, files in os.walk(filepath):\n            try:\n                for name in files:\n\n                    corpus = ET.parse(os.path.join(path, name)).getroot()\n\n                    #from the root, get all children that follow this node heirarchy, two different types of text blocks so two different for loops\n                    # print(len(corpus.findall('par/sen')))\n                    # print(len(corpus.findall('sec/par/sen')))\n\n                    for type_tag in corpus.findall('par/sen'):\n                        value = list(type_tag) # get all children elements under the <sen> tag\n                        japanese_child = value[0].text # 1st child\n                        english_child = value[5].text # 6th child\n\n                        if english_child and japanese_child:\n                            if (\"(\" in english_child and \")\" in english_child) and not (\"(\" in japanese_child and \")\" in japanese_child):\n                                continue\n                            file_in.write(english_child + \"\\t\" + japanese_child + \"\\n\")\n\n\n                    for type_tag in corpus.findall('sec/par/sen'):\n                        value = list(type_tag) # get all children elements under the <sen> tag\n                        japanese_child = value[0].text # 1st child\n                        english_child = value[5].text # 6th child\n\n                        if english_child and japanese_child:\n                            if (\"(\" in english_child and \")\" in english_child) and not (\"(\" in japanese_child and \")\" in japanese_child):\n                                continue\n                            file_in.write(english_child + \"\\t\" + japanese_child + \"\\n\")\n            except:\n                pass\n    print('tsv saved!')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create wikipedia tsv\n# createEnglishJapanesePairList('../input/japaneseenglish-bilingual-corpus/wiki_corpus_2.01', 'wikipedia_raw.tsv')","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combine Stanford Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_stanford_dataset(dataset_path, output):\n    with open(output, mode=\"w\", encoding=\"utf-8\") as file_write:\n        with open(dataset_path + \"/train\", mode=\"r\", encoding=\"utf-8\") as file_train:\n            for index, line in enumerate(file_train):\n                if index > 0:\n                    file_write.writelines(line)\n        with open(dataset_path + \"/test\", mode=\"r\", encoding=\"utf-8\") as file_test:\n            for index, line in enumerate(file_test):\n                if index > 0:\n                    file_write.writelines(line)\n        with open(dataset_path + \"/dev\", mode=\"r\", encoding=\"utf-8\") as file_dev:\n            for index, line in enumerate(file_dev):\n                if index > 0:\n                    file_write.writelines(line)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine stanford dataset\n# combine_stanford_dataset(\"../input/japanesse-english-subtittle-corpusjesc-cleaned\", \"stanford_raw.tsv\")","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Vectors for Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"SOS_token = \"<SOS>\"\nEOS_token = \"<EOS>\"\ntagger = fugashi.Tagger()\n\n# generates sentences for gensim's word2vec encoder\n# uses fugashi to tokenize japanese text\nclass weebCorpus:\n    filename = \"\"\n    language = \"japanese\"\n    \n    def __init__(self, c, l):\n        self.corpus = c\n        self.language = l\n    \n    def __iter__(self):\n        tokenized_text = list()\n        for file in self.corpus:\n            for line in open(file, mode=\"r\", encoding=\"utf-8\"):\n                if self.language == \"japanese\":\n                    text = line.split(\"\\t\")[1]\n                    tokenized_text = [SOS_token] + [word.surface for word in tagger(text)] + [EOS_token]\n                else: # english\n                    text = line.split(\"\\t\")[0]\n                    tokenized_text = [SOS_token] + nltk.word_tokenize(text)\n                    tokenized_text.append(EOS_token)\n                yield tokenized_text\n\ndef genVectors(corpus, language = \"japanese\"):\n    sentences = weebCorpus(corpus, language)\n    model = Word2Vec(sentences=sentences,\n                     size=300,\n                     window=5,\n                     workers=2, # 2 if using GPU on Kaggle, 4+ if not\n                     min_count=5,\n                     iter=5)\n        \n    model.save(\"./\" + language + \"_vectors.model\")\n    # model.wv.save_word2vec_format(\"./\" + language + \"vectors_readable.txt\", binary=False)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_dir = \"../input/vectorized-jesc-wikipedia-raw\"\n\nif not os.path.exists(pretrained_dir):\n    # start time to calculate time it takes to generate vectors\n    start = time.time()\n    files = [\"stanford_raw.tsv\", \"wikipedia_raw.tsv\"]\n\n    genVectors(files, language=\"japanese\")\n    genVectors(files, language=\"english\")\n\n    print(time.time() - start)\n    pretrained_dir = \"\"\n    \n \nprint(\"most similar to 時代: \")\nw1 = [\"時代\"]\njapW2V = gensim.models.Word2Vec.load(os.path.join(pretrained_dir, \"japanese_vectors.model\"))\nfor thing in japW2V.wv.most_similar(positive=w1,topn=6):\n    print (thing[0])\n\nprint(\"most similar to rice: \")\nw1 = [\"rice\"]\nengW2V = gensim.models.Word2Vec.load(os.path.join(pretrained_dir, \"english_vectors.model\"))\nfor thing in engW2V.wv.most_similar(positive=w1,topn=6):\n    print (thing[0])","execution_count":8,"outputs":[{"output_type":"stream","text":"most similar to 時代: \n中期\n後期\n期\n前期\n近世\n末期\nmost similar to rice: \nwheat\nsoup\nsauce\nvegetables\nmeat\nmochi\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Initialize CUDA and load Generated Word Vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizeJapanese(string):\n    return [word.surface for word in tagger(string)]","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, embedding_size, hidden_size, num_layers, dropout): # input_size = number of tokens in Japanese, hidden_size=rand number\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(japW2V.wv.vectors))\n        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input):\n        embedded = self.dropout(self.embedding(input))\n        output, (hidden_state, cell_state) = self.lstm(embedded)\n        \n        return hidden_state, cell_state","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, embedding_size, hidden_size, output_size, num_layers, dropout): # hidden_size=rand number (same that EncoderDNN() used, output_size = total tokens of English\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.dropout = nn.Dropout(dropout)\n        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(engW2V.wv.vectors))\n        self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input, hidden_state, cell_state):\n        x = input.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden_state, cell_state","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"JAPANESE = Field(tokenize=tokenizeJapanese,\n               lower=True,\n               init_token=SOS_token,\n               eos_token=EOS_token)\n\nENGLISH = Field(tokenize=nltk.tokenize.word_tokenize,\n               lower=True,\n               init_token=SOS_token,\n               eos_token=EOS_token)\n\nfields = [\n  ('en', ENGLISH),\n  ('ja', JAPANESE)\n]\n\nstanford_data = TabularDataset(\n   path = os.path.join(pretrained_dir, 'stanford_raw.tsv'),\n   format = 'tsv',\n   fields = fields,\n   skip_header = False\n)\n\nstanford_train, stanford_test = stanford_data.split(split_ratio=0.9)","execution_count":13,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Number of training examples: {len(stanford_train.examples)}\")\nprint(f\"Number of test examples: {len(stanford_test.examples)}\")\n\nJAPANESE.build_vocab(stanford_train, min_freq=3)\nENGLISH.build_vocab(stanford_test, min_freq=3)\n\nprint(f\"Unique tokens in source (japanese) vocabulary: {len(JAPANESE.vocab)}\")\nprint(f\"Unique tokens in target (english) vocabulary: {len(ENGLISH.vocab)}\")","execution_count":14,"outputs":[{"output_type":"stream","text":"Number of training examples: 2138314\nNumber of test examples: 237591\nUnique tokens in source (japanese) vocabulary: 60917\nUnique tokens in target (english) vocabulary: 20189\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_iterator, valid_iterator = BucketIterator.splits(\n    (stanford_train, stanford_test),\n    batch_size = 128,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.ja),\n    device = device\n)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoder/decoder params\nembedding_size = 300\nhidden_size = 1024\nnum_layers = 2\ndropout = 0.5\noutput_size = len(ENGLISH.vocab)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_lstm = EncoderRNN(embedding_size, hidden_size, num_layers, dropout).to(device)\ndecoder_lstm = DecoderRNN(embedding_size, hidden_size, output_size, num_layers, dropout).to(device)","execution_count":29,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoder to Decoder Module"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, Encoder, Decoder):\n        super(Seq2Seq, self).__init__()\n        self.Encoder = Encoder\n        self.Decoder = Decoder\n\n    def forward(self, source, target, tfr=0.5):\n        # Shape - Source : (10, 32) [(Sentence length German + some padding), Number of Sentences]\n        batch_size = source.shape[1]\n\n        # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n        target_len = target.shape[0]\n        target_vocab_size = len(ENGLISH.vocab)\n\n        # Shape --> outputs (14, 32, 5766)\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n\n        # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n        hidden_state, cell_state = self.Encoder(source)\n\n        # Shape of x (32 elements)\n        x = target[0]  # Trigger token <SOS>\n\n        for i in range(1, target_len):\n            # Shape --> output (32, 5766)\n            output, hidden_state, cell_state = self.Decoder(x, hidden_state, cell_state)\n            outputs[i] = output\n            best_guess = output.argmax(1)  # 0th dimension is batch size, 1st dimension is word embedding\n            x = target[i] if random.random() < tfr else best_guess  # Either pass the next word correctly from the dataset or use the earlier predicted word\n\n        # Shape --> outputs (14, 32, 5766)\n        return outputs","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize main model and optimizer\nmodel = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def translate_sentence(model, sentence, japanese, english, device, max_length=50):\n    tokens = [japanese.init_token]\n    if type(sentence) == str:\n        tokens.extend(tokenizeJapanese(sentence))\n    else:\n        tokens.extend([token.lower() for token in sentence])\n    tokens.append(japanese.eos_token)\n    text_to_indices = [japanese.vocab.stoi[token] for token in tokens]\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.Encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[SOS_token]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.Decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[EOS_token]:\n            break\n\n        outputs.append(best_guess)\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n    return translated_sentence[1:]","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bleu(data, model, japanese, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"ja\"]\n        trg = vars(example)[\"en\"]\n        \n        prediction = translate_sentence(model, src, japanese, english, device)\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def checkpoint_and_save(model, best_loss, epoch, optimizer):\n    print('saving')\n    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n    torch.save(state, 'checkpoint')\n    torch.save(model.state_dict(),'checkpoint-sd')","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_idx = ENGLISH.vocab.stoi[\"<pad>\"]\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, optimizer, num_epochs, example_sentence):\n    epoch_loss = 0.0\n    best_loss = math.inf\n    best_epoch = -math.inf\n    for epoch in range(num_epochs):\n        print(\"Epoch - {} / {}\".format(epoch + 1, num_epochs))\n        model.eval()\n        translated_sentence = translate_sentence(model, example_sentence, JAPANESE, ENGLISH, device, 50)\n        print(f\"Translated example sentence: \\n {translated_sentence}\\n\")\n\n        model.train(True)\n        for batch_idx, batch in enumerate(train_iterator):\n            input = batch.ja.to(device)\n            target = batch.en.to(device)\n\n            # Pass the input and target for model's forward method\n            output = model(input, target)\n            output = output[1:].reshape(-1, output.shape[2])\n            target = target[1:].reshape(-1)\n\n            # Clear the accumulating gradients\n            optimizer.zero_grad()\n\n            # Calculate the loss value for every epoch\n            loss = criterion(output, target)\n\n            # Calculate the gradients for weights & biases using back-propagation\n            loss.backward()\n\n            # Clip the gradient value if it exceeds > 1\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n            # Update the weights values using the gradients we calculated using bp\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        if loss.item() < best_loss:\n            best_loss = loss.item()\n            best_epoch = epoch\n            checkpoint_and_save(model, best_loss, epoch, optimizer) \n        elif (epoch - best_epoch) >= 10:\n            print(\"no improvement in 10 epochs, break\")\n            break\n\n        print(\"Epoch_Loss - {}\".format(loss.item()))\n\n    print(epoch_loss / len(train_iterator))\n\n    score = bleu(test_ds[:100], model, JAPANESE, ENGLISH, device)\n    print(f'BLEU score: {score*100:.2f}')","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = \"今お前などに興味はない。\" # i am not interested in you now.\ntrain(model, optimizer, 200, sentence)","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch - 1 / 200\nTranslated example sentence: \n ['makeup', 'makeup', 'makeup', 'makeup', 'spacetime', 'spacetime', 'makeup', 'makeup', 'spacetime', 'makeup', 'spacetime', 'makeup', 'spacetime', 'makeup', 'makeup', 'spacetime', 'spacetime', 'makeup', 'makeup', 'spacetime', 'makeup', 'spacetime', 'makeup', 'spacetime', 'makeup', 'makeup', 'spacetime', 'spacetime', 'makeup', 'makeup', 'spacetime', 'makeup', 'spacetime', 'makeup', 'spacetime', 'makeup', 'makeup', 'spacetime', 'spacetime', 'makeup', 'makeup', 'spacetime', 'makeup', 'spacetime', 'makeup', 'spacetime', 'makeup', 'makeup', 'spacetime', 'spacetime']\n\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}